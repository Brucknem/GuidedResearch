% !TEX root=./report.tex

\section{Future Work}
The project leaves us with the opportunity to continue the research in multiple directions.

\subsection{Varying Weather and Lighting Conditions}
We tested and evaluated the implementations on recordings with good weather and lighting conditions, thus a next step is to test the implementations in bad weather and lighting conditions, \eg by night, rain and snow.
From out current perspective the feature based dynamic stabilization approach will suffer in performance as the homography estimation depend on features in the image space. 
By night and if the static background is occluded the stabilization pipeline will fail, although in these cases the complete RGB image will be unusable at all.

We implemented the solver for the BA problem to include human interaction when mapping from PDs to pixels.
The mapping will be harder in bad weather and lighting conditions based on the worse visibility of the landmarks.
We propose an automatic mapping scheme in \Cref{sec:auto_mapping_landmarks}.
This scheme will be affected by changing weather and lighting conditions as the detection of new landmarks is also based on the visibility of landmarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dynamic Stabilization}
We present two major improvements that can be done to extend the presented dynamic stabilization approach.

\subsubsection{Warp Field Stabilization Based on Optical Flow}
We use the optical flow to measure the performance of the stabilizers as described in  \Cref{sec:evaluation_dynamic_stabilization_optical_flow}.
The optical flow is a 2D vector field where each vector is a displacement vector showing the movement of pixels between frames caused by movement of the objects or cameras.
The image can be stabilized using the inverse vector field that also minimizes the reprojection error between frames.

\subsubsection{Deep Learning Based Dynamic Stabilization}
Based on the ongoing success of deep learning approaches in computer vision, especially of convolutional neural networks (CNN), a self-learning stabilization procedure might be developed.
The CNN expects the current input and reference frame and outputs the homographic transformation or the warped frame. 
This might speed up the pipeline and inherently adds a measure for the uncertainty of the results by modelling the probability of the homographic transformation.
This approach can be used to fuse the feature detection, matching and warping steps into one joint step that is learned by the CNN from labeled data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Static Calibration}
We present two major improvements that can be done to extend the presented static calibration approach.

\subsubsection{Robustness Against Outliers}
BA problems are inherently prone to outliers, as they greatly impact the shape of the reprojection-error loss landscape. 
To use the presented system in practice a RANSAC \cite{fischler1981random} or similar sample consensus based approach needs to be implemented.
By evaluating the calibration multiple times with different subsets of correspondences a stable sample consensus can be found in asymptotically all runs.
This will greatly impact the robustness of the calibration procedure against outliers, as they can be filtered out automatically by the algorithm.  


\subsubsection{Fully Automatic Static Calibration}
\label{sec:auto_mapping_landmarks}
We establish the mapping of the correspondences by hand, thus a human has to look up the ids of the landmarks in the HD maps and assign them to their respective pixels.
After an initial calibration that requires human interaction, an image region based approach might be used to automate this mapping.
One can project the known base origin points of the objects from the HD maps into the current frame.
Starting from the projected pixel locations one could search in a defined enclosing region to find pixels that clearly correspond to the objects by applying template, color or gradient matching approaches.
The automatic detection of landmarks enables the system to perform fully automatic static self calibration.

\subsubsection{Machine Learning Based Bundle Adjustment}
Aravkin \etal \cite{students_t_bundle_adjustment} have shown that the BA problem can be modelled on top of a Student's-t distribution. 
The resulting statistical machine learning approach for the BA can be used to jointly estimate the camera parameters and world positions of objects, while at the same time being robust against outliers.
  
\subsubsection{New High Definition Map}
The newer OpenDRIVE standard also provides the possibility to include lane markings.
These lane markings are easily detectable and can be used for the calibration procedure in conjunction with the object landmarks.
This would greatly simplify the automatic detection and mapping procedures as described in \Cref{sec:auto_mapping_landmarks} as they are spatially more extend and thus easier to detect.
Furthermore, the lane markings are always white or yellow which simplifies the detection.
As shown in \Cref{sec:static_calibration_expectable_error} the static calibration is more precise the closer the correspondences are to the cameras. 
As there exists far more lane markings than objects, and with the markings being evenly distributed over the road the static calibration would benefit from this additional information.