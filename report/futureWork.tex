% !TEX root=./report.tex

\section{Future Work}
The project leaves us with the opportunity to continue the research in multiple directions.

\subsection{Test on different weather/lighting conditions}
We tested and evaluated the implementations on recordings with good weather and lighting conditions, thus a next step is to test the implementations in bad weather and lighting conditions, \eg by night, rain and snow.
From out current perspective the feature based dynamic stabilization approach will not suffer in performance as the detected features only depend on features in the image space. 
Only by night and if the static background is occluded the stabilization pipeline will fail.

We implemented the solver for the BA problem to include human interaction when mapping from PDs to pixels.
The mapping will be harder in bad weather and lighting conditions based on the worse visibility of the landmarks.
We propose an automatic mapping scheme in \autoref{sec:auto_mapping_landmarks}.
This scheme will be affected by changing weather and lighting conditions as the detection of new landmarks is also based on the visibility of landmarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dynamic Stabilization}
In the subfield of the dynamic stabilization we have until now implemented the visual feature based approach.

\subsubsection{Warp Field Stabilization based on Optical Flow}
We use the optical flow to measure the performance of the stabilizers as described in  \autoref{sec:evaluation_dynamic_stabilization_optical_flow}.
The optical flow is a 2D vector field where each vector is a displacement vector showing the movement of pixels between frames caused by movement of the objects or cameras.
The image can be stabilized using the inverse vector field that also minimizes the reprojection error between frames.

\subsubsection{Deep Learning based approaches}
The ongoing success of deep learning based approaches in computer vision, especially with convolutional neural networks (CNN), a self-learning stabilization procedure might be developed.
The CNN expects the current input and reference frame and outputs the homographic transform or the warped frame. 
This might speed up the pipeline and inherently adds a measure for the uncertainty of the results by modelling the probability of a homographic transformation.
This approach can be used to fuse the feature detection, matching and warping steps into one joint step that is learned by the CNN from labeled data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Static Calibration}
For the static calibration there are several improvements possible.

\subsubsection{IMU based sensor fusion}
Previous by-hand calibrations have shown that an inertia measurement unit (IMU) can be used to initially find a camera pose, but the lack of performance and huge time needed makes this not feasible.
This is just empirical knowledge, not backed up by research. 
This should be looked into further.


\subsubsection{Automatic detection of more landmarks after initial calibration}
\label{sec:auto_detection_landmarks}
Currently the detection and mapping of landmarks is done by hand using a watershed algorithm \cite{meyer1992color,opencv_library} based external tool.
This tool relies on a by hand marking of landmarks as the markers used to find the segmentation.
By applying template, color or gradient matching approaches the detection of the markers might be automated.
This automated detection would speed up the mapping procedure. 

\subsubsection{Automatic mapping of pixels to objects}
\label{sec:auto_mapping_landmarks}
To establish the mapping of the correspondences a human has to look up the ids of the landmarks in the \HDmaps{}.
After an initial calibration an image region based approach might be used to automate this mapping.
This positions of the known objects from the \HDmaps{} can be projected into the camera image with the current camera pose.
Starting from the resulting pixel locations one could search in a defined enclosing region to search for pixels that clearly correspond to the objects.
This automatic detection of more landmarks could further improve the robustness an can be used to mitigate drift in the cameras.  

\subsubsection{Machine learning based pose estimation}
A machine learning based approach for the bundle adjustment problem we faced and the resulting camera pose estimation problem can be formulated.
As Aravkin \etal \cite{students_t_bundle_adjustment} have shown a Student's-t distribution based approach can be used to estimate and robustify the procedure against outliers.
  
\subsubsection{New HD map}
The newer OpenDRIVE standards also provide the possibility to include lane markings.
These lane markings are easily detectable and can then be used for the calibration procedure in conjunction with the object landmarks.
This would greatly simplify the automatic detection and mapping procedures as described in Sec. \ref{sec:auto_detection_landmarks} and Sec. \ref{sec:auto_mapping_landmarks} as they are spatially more extend and thus easier to detect.
Additionally with the color information, as the lane markings are always white (Germany) and yellow (USA) the detection is simplified a lot.